\documentclass[11pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{titlesec}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{courier}
\usepackage{xcolor}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    urlcolor=blue
}

\pagestyle{fancy}
\fancyhf{}
\rhead{RAG Assistant}
\lhead{LLM-Powered Knowledge Assistant}
\rfoot{\thepage}

\title{\textbf{RAG Assistant: LLM-Powered Knowledge Assistant}\\
\vspace{0.5em}
\large Retrieval-Augmented Generation System using OpenAI, FAISS, and FastAPI}

\author{Your Name}
\date{\today}

\begin{document}

\maketitle

\section{Overview}

RAG Assistant is a Retrieval-Augmented Generation (RAG) system that enables semantic search and grounded question answering over PDF documents using large language models and vector similarity search.

The system combines document embeddings, vector retrieval, and generative AI to produce accurate, explainable answers with source citations and confidence scores.

This project demonstrates a complete end-to-end implementation of modern production-grade AI retrieval architecture.

\section{Key Features}

\begin{itemize}[leftmargin=*]
\item Semantic search over PDF documents using OpenAI embeddings
\item FAISS vector database for efficient similarity search
\item Retrieval-Augmented Generation (RAG) pipeline
\item Grounded answers with source citations and confidence scores
\item FastAPI backend exposing RESTful API endpoints
\item Interactive browser-based chat interface
\item Chunk-level metadata tracking and explainability
\item Modular and scalable system architecture
\end{itemize}

\section{System Architecture}

The system pipeline consists of the following components:

\begin{enumerate}[leftmargin=*]
\item PDF document ingestion
\item Text preprocessing and chunking
\item Embedding generation using OpenAI embedding model
\item Vector indexing using FAISS
\item Semantic similarity retrieval
\item Context construction from retrieved chunks
\item Answer generation using GPT-4o-mini
\item Response returned with citations and confidence metrics
\end{enumerate}

\section{Example Output}

An example response generated by the system is shown below:

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{demo/demo.png}
\caption{Example grounded response with source citations}
\end{figure}

The system retrieves relevant document chunks and generates an answer grounded in the source material.

Each response includes:

\begin{itemize}
\item Answer generated by the language model
\item Source file and page number
\item Similarity score and confidence estimate
\item Supporting text snippets
\end{itemize}

\section{Technology Stack}

\begin{itemize}[leftmargin=*]
\item Python
\item FastAPI
\item LangChain
\item OpenAI API
\item FAISS Vector Database
\item PyPDF Document Loader
\item HTML / JavaScript frontend
\item Docker (optional deployment)
\end{itemize}

\section{Project Structure}

\begin{verbatim}
RAG-Assistant/
│
├── app/
│   ├── main.py
│   ├── ingest.py
│   └── ui.html
│
├── data/
│   └── raw_docs/
│
├── docs/
│   └── demo/
│
├── requirements.txt
├── README.tex
└── .gitignore
\end{verbatim}

\section{Installation}

\subsection{Clone repository}

\begin{verbatim}
git clone https://github.com/YOUR_USERNAME/RAG-Assistant.git
cd RAG-Assistant
\end{verbatim}

\subsection{Create virtual environment}

\begin{verbatim}
python -m venv .venv
.venv\Scripts\activate
\end{verbatim}

\subsection{Install dependencies}

\begin{verbatim}
pip install -r requirements.txt
\end{verbatim}

\subsection{Set OpenAI API key}

Create .env file:

\begin{verbatim}
OPENAI_API_KEY=your_api_key_here
\end{verbatim}

\section{Usage}

\subsection{Add documents}

Place PDF files in:

\begin{verbatim}
data/raw_docs/
\end{verbatim}

\subsection{Build vector index}

\begin{verbatim}
python app/ingest.py
\end{verbatim}

\subsection{Run server}

\begin{verbatim}
uvicorn app.main:app --reload
\end{verbatim}

\subsection{Open chat interface}

\begin{verbatim}
http://127.0.0.1:8000/ui
\end{verbatim}

\section{API Endpoints}

\begin{itemize}
\item \textbf{/chat} – Query documents using natural language
\item \textbf{/files} – List indexed PDF documents
\item \textbf{/health} – System health check
\item \textbf{/ui} – Interactive chat interface
\item \textbf{/docs} – Swagger API documentation
\end{itemize}

\section{Key Concepts Demonstrated}

This project demonstrates the following core AI and backend engineering concepts:

\begin{itemize}
\item Retrieval-Augmented Generation (RAG)
\item Vector similarity search
\item Semantic embeddings
\item Large Language Model integration
\item Backend API architecture
\item Explainable AI responses with citations
\item Scalable AI system design
\end{itemize}

\section{Future Improvements}

Potential enhancements include:

\begin{itemize}
\item Streaming responses
\item Hybrid search (BM25 + vector search)
\item Document upload endpoint
\item Authentication and access control
\item Cloud deployment
\item Persistent database storage
\end{itemize}

\section{Conclusion}

This project demonstrates a production-style implementation of a Retrieval-Augmented Generation system integrating vector databases, large language models, and scalable backend architecture.

The system provides accurate, explainable AI-generated answers grounded in source documents, representing a core architecture used in modern AI applications such as enterprise search assistants and knowledge systems.

\end{document}
